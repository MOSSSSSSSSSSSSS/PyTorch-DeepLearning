在01中w的取值是靠穷举范围内每个w再计算mse，绘图后得到的                     
在02中使用梯度下降法，w = w - a * (d(cost) / dw)得到，a是学习率，应该取较小值否则可能不收敛，最终w不一定是全局最优解               
&emsp;&emsp;只能是局部最优解（极小值点）(非凸函数，即以函数上两点画一条线，不能保证线上所有点在曲面上方，这种情况下可能求得局部最优解)               
因为一般不会出现太多局部最优点，因为梯度下降方法用途广泛，但是有鞍点，鞍点处梯度为0无法继续迭代              
更常用的是stochastic gradient descent随机梯度下降，不再是用全部样本（x1,x2,x3...）相加求均值的cost，而是从所有             
&emsp;&emsp;的里面每次选择一个，用d(loss)/dw计算梯度，这样由于样本的噪声有可能跨越过鞍点           
梯度下降算法可以并行化运算，因此用时更少，但性能更差           
随机梯度下降算法不可以并行化运算，因此用时更多，但性能更好          
折中的批量的随机梯度下降算法，用一个把整个数据集划分为若干个批量(mini-batch，本来batch是指整个数据集，而现在batch是指mini-batch)，每次用一个批量里的计算梯度，使性能和用时适中               
![](02.GradientDescent/Graph1.jpg)
![](02.GradientDescent/Graph2.jpg)
